{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvnx9tw3Od9B"
   },
   "source": [
    "### **Importing Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n8oWUJa198qQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ci_YmDqdlLEK"
   },
   "source": [
    "### **Loading Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "ehSukdgE_hQG",
    "outputId": "46e51fff-b931-4c4d-92d5-00b89e46f5cb"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"E:\\\\SCP Judgements\\\\Small Corpus (Metadata incl.)\\\\CSV\\\\SCP_Judgements_with_Metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXYEsp16lOhM"
   },
   "source": [
    "### **Text Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VwGtRd_MOV1v"
   },
   "outputs": [],
   "source": [
    "# Define preprocessing functions\n",
    "def remove_rows_with_urdu_title(df):\n",
    "    # Convert titles to lowercase and check if \"urdu\" or \"translation\" is present\n",
    "    mask = df['title'].str.lower().str.contains('urdu|translation')\n",
    "    # Remove rows where the mask is True\n",
    "    return df[~mask]\n",
    "\n",
    "def remove_non_textual_elements(text):\n",
    "    # Remove page numbers, non-English characters, etc.\n",
    "    text = re.sub(r'\\bPage \\d+\\b', '', text)  # Removes \"Page X\" if present\n",
    "    text = re.sub(r'[^\\x00-\\x7F\\n]+', ' ', text)  # Retain newlines while removing other non-ASCII characters\n",
    "    return text\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    # Replace two or more spaces with a single space\n",
    "    text = re.sub(r' {2,}', ' ', text)\n",
    "    # Additional step: Replace ' \\n ' with '\\n'\n",
    "    text = text.replace(' \\n', '\\n')\n",
    "    text = text.replace('\\n ', '\\n')\n",
    "    # Limit line breaks to a maximum of five consecutive line breaks\n",
    "    text = re.sub(r'(\\n{5,})', '\\n\\n\\n\\n', text)\n",
    "\n",
    "    return text  # Remove leading/trailing whitespace\n",
    "\n",
    "def deduplicate_phrases(text):\n",
    "    # Define a list of redundant phrases to remove\n",
    "    redundant_phrases = [\n",
    "        \"in the supreme court of pakistan\",\n",
    "        \"approved for reporting\",\n",
    "        \"in the court of law\"\n",
    "    ]\n",
    "\n",
    "    # Create a regex pattern from the phrases\n",
    "    pattern = r'\\b(?:' + '|'.join(map(re.escape, redundant_phrases)) + r')\\b'\n",
    "\n",
    "    # Replace redundant phrases with an empty string\n",
    "    text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove extra whitespace but keep newlines intact\n",
    "    text = re.sub(r' +', ' ', text)  # Replace multiple spaces with a single space\n",
    "    return text.strip()  # Keep leading/trailing spaces without affecting newlines\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = remove_non_textual_elements(text)\n",
    "    text = normalize_whitespace(text)\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = deduplicate_phrases(text)\n",
    "    return text\n",
    "\n",
    "# Manually fill one missing date\n",
    "df.iloc[263,5]=\"28-11-2022\"\n",
    "\n",
    "# Removing Completely/Partially Urdu Documents\n",
    "df=remove_rows_with_urdu_title(df)\n",
    "\n",
    "# Apply preprocessing to the 'content' column\n",
    "df['Content'] = df['content'].apply(preprocess_text)\n",
    "\n",
    "# Lower casing the metadata column content\n",
    "df['Case Subject']=df['Case Subject'].str.lower().copy()\n",
    "df['Case No']=df['Case No'].str.lower().copy()\n",
    "df['Case Title']=df['Case Title'].str.lower().copy()\n",
    "df['Author Judge']=df['Author Judge'].str.lower().copy()\n",
    "\n",
    "# Drop the unnecessary columns\n",
    "df.drop(columns=['title','content'], inplace=True)\n",
    "\n",
    "# Lower casing the column names\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# Reset index\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkiyHG65lfBT"
   },
   "source": [
    "**Data Cleaning Steps:**\n",
    "* Removing rows with title containing 'Urdu'|'Translation' as it'll have 'Urdu'/'Arabic' text content which is outside the scope of this project.\n",
    "* Removing non-textual elements from content like page numbers (identified by the pattern Page X) and Non-ASCII characters (keeps only standard English letters, digits, and newlines).\n",
    "* Normalizing Whitespace:\n",
    "  * Replace multiple spaces with a single space.\n",
    "  * Replace newline characters that are surrounded by spaces ( \\n ) with just \\n.\n",
    "  * Limit consecutive line breaks to no more than five consecutive line breaks, reducing excessive empty lines.\n",
    "* Removing specific redundant phrases (like \"in the supreme court of pakistan,\" \"approved for reporting,\" and \"in the court of law\").\n",
    "* Manually filling in a missing value for a judgment date at index 263 by specifying \"28-11-2022\" which is hearing date mentioned in the judgement. This metadata was N/A on the website itself from where the document is scrapped.\n",
    "* Lowercasing all entries including the column names and dropping redundant columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OToXjTezOoxV"
   },
   "source": [
    "### **Downloading Cleaned Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "eSp7Fj49Tr-K",
    "outputId": "d6cb9e8d-ab42-4146-afd0-ac2276d39ec2"
   },
   "outputs": [],
   "source": [
    "df.to_csv('E:\\\\SCP Judgements\\\\Small Corpus (Metadata incl.)\\\\Cleaned Corpus\\\\SCP_Judgements_Cleaned_with_Metadata.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
